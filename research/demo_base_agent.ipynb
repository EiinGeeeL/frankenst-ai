{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad32497",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current research's directory\n",
    "research_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "# Move one directory back\n",
    "parent_dir = os.path.dirname(research_dir)\n",
    "\n",
    "# Change the current working directory to the parent directory\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "# Print the current working directory to confirm\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05066a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frank.utils.common import print_process_astream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46173290",
   "metadata": {},
   "source": [
    "# Base Agent in LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6f545",
   "metadata": {},
   "source": [
    "#### LLM Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdfc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.ai_foundry.llm import LLMServices\n",
    "\n",
    "LLMServices.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f09b1",
   "metadata": {},
   "source": [
    "#### TOOLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf13573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "from typing import List\n",
    "from langchain_core.tools import ToolException\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_evolution(pokemon_name: str) -> list:\n",
    "    \"\"\"This is a method to give you a information of the evolution path of a certain pokemon\n",
    "\n",
    "    Args:\n",
    "        pokemon_name: a pokemon name given by the user.\n",
    "    \"\"\"\n",
    "\n",
    "    species_url = f\"https://pokeapi.co/api/v2/pokemon-species/{pokemon_name.lower()}\"\n",
    "    species_response = requests.get(species_url)\n",
    "\n",
    "    if species_response.status_code != 200:\n",
    "        raise ToolException(f\"Error: {pokemon_name} is not a valid pokemon\")\n",
    "    \n",
    "    species_data = species_response.json()\n",
    "\n",
    "    # Step 2: Extract evolution chain URL from species data\n",
    "    evolution_chain_url = species_data['evolution_chain']['url']\n",
    "\n",
    "    # Step 3: Get the evolution chain data\n",
    "    evolution_response = requests.get(evolution_chain_url)\n",
    "    evolution_data = evolution_response.json()\n",
    "\n",
    "    # Step 4: Traverse the evolution chain and get the names of evolutions\n",
    "    evolutions = []\n",
    "    current_evolution = evolution_data['chain']\n",
    "    \n",
    "    while current_evolution:\n",
    "        evolutions.append(current_evolution['species']['name'])\n",
    "        if len(current_evolution['evolves_to']) > 0:\n",
    "            current_evolution = current_evolution['evolves_to'][0]\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return evolutions\n",
    "\n",
    "@tool\n",
    "def random_movements(pokemon_name: str) -> List[str]:\n",
    "    \"\"\"This is a method to give you a random movements list of a certain pokemon if the user asks for them\n",
    "\n",
    "    Args:\n",
    "        pokemon_name: a pokemon name given by the user.\n",
    "    \"\"\"\n",
    "\n",
    "    # The url of the api\n",
    "    url = f'https://pokeapi.co/api/v2/pokemon/{pokemon_name.lower()}'\n",
    "        \n",
    "    # Make the API request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        raise ToolException(f\"Error: {pokemon_name} is not a valid pokemon\")\n",
    "\n",
    "    # Parse the response JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the list of moves using map and lambda\n",
    "    moves = list(map(lambda move: move['move']['name'], data['moves']))\n",
    "\n",
    "    if len(moves) < 4:\n",
    "        return moves\n",
    "\n",
    "    # Select 4 random\n",
    "    selected_moves = random.sample(moves, 4)\n",
    "\n",
    "    return selected_moves\n",
    "\n",
    "tools = [get_evolution, random_movements]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94f33c",
   "metadata": {},
   "source": [
    "#### PROMPT & CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdbf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "system_prompt=\"\"\"\n",
    "<context>\n",
    "You are Professor Oak, a world-renowned Pokémon Professor from Pallet Town. Your expertise lies exclusively in Pokémon, \n",
    "and you have very limited knowledge of real-world animals.\n",
    "</context>\n",
    "\n",
    "<instructions> \n",
    "1. If you know the answer, respond confidently and clearly.\n",
    "2. Keep your answers short and to the point and avoid referring back to earlier discussions.\n",
    "3. Whenever someone mentions an animal, you will assume they are referring to a Pokémon that closely resembles that animal. \n",
    "4. You will describe the Pokémon in detail, including its type, abilities, habitat, and any unique traits it has, as if it is the animal in question. \n",
    "5. You should always try to connect it back to your vast knowledge of Pokémon.\n",
    "</instructions>\n",
    "    \n",
    "<input>\n",
    "strings\n",
    "</input>\n",
    "    \n",
    "<output_format>\n",
    "strings\n",
    "</output_format>\n",
    "\n",
    "<restrictions>\n",
    "1. You dont know about real animals—only Pokémon.\n",
    "</restrictions>\n",
    "\"\"\"\n",
    "\n",
    "history_template = [\n",
    "    (\"human\", \"Professor Oak, I want to be Pokémon Trainer and catch 'em all!\"), \n",
    "    (\"ai\", \"\"\"Ah, so you want to become a Pokémon Trainer, do you? That's quite the ambitious goal!\n",
    "    First things first, you will need a partner. Have you thought about which starter Pokémon you want to choose?\"\"\"),\n",
    "]\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            # few_shot_prompt,\n",
    "            history_template[0],\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ])\n",
    "\n",
    "\n",
    "model_with_tools = LLMServices.model.bind_tools(tools)\n",
    "\n",
    "chain = prompt_template | model_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model without langgraph\n",
    "# Define the input\n",
    "user_input = \"Hello, how are you?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "\n",
    "chain.invoke(message_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "user_input = \"What is the evolution of Pikachu?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "\n",
    "chain.invoke(message_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5e9c3",
   "metadata": {},
   "source": [
    "#### GRAPH COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c304e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# NOTE: this is a method 'from langgraph.prebuilt import tools_condition'\n",
    "def tools_condition(state, messages_key: str = \"messages\") -> Literal[\"end\", \"tools\"]:\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif isinstance(state, dict) and (messages := state.get(messages_key, [])):\n",
    "        ai_message = messages[-1]\n",
    "    elif messages := getattr(state, messages_key, []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return \"end\"\n",
    "\n",
    "def call_model_chained(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = chain.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a85cec",
   "metadata": {},
   "source": [
    "#### GRAPH COMPILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the function to execute tools\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Define the checkpointer\n",
    "memory = InMemorySaver()\n",
    "\n",
    "# Nodes\n",
    "workflow.add_node(\"OakLangAgent\", call_model_chained)\n",
    "workflow.add_node(\"OakTools\", tool_node)\n",
    "\n",
    "# Edges\n",
    "workflow.add_edge(START, \"OakLangAgent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"OakLangAgent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"OakTools\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"OakTools\", \"OakLangAgent\")\n",
    "\n",
    "# Finally, we compile it!\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "user_input = \"Hi!\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "await print_process_astream(graph, message_input, runnable_config={\"configurable\": {\"thread_id\": \"001\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "user_input = \"Could you give me random movements of pickachu, and what is his evolutions?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "await print_process_astream(graph, message_input, runnable_config={\"configurable\": {\"thread_id\": \"001\"},})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "user_input = \"Please, give me random movements of that evolution?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "await print_process_astream(graph, message_input, runnable_config={\"configurable\": {\"thread_id\": \"001\"},})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08a0cb",
   "metadata": {},
   "source": [
    "# Base Agent in LangGraph + Frank utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from frank.workflow_builder import WorkflowBuilder\n",
    "from frank.config.layouts.simple_oak_config_graph import SimpleOakConfigGraph\n",
    "from frank.models.stategraph.stategraph import SharedState\n",
    "from frank.utils.common import read_yaml\n",
    "from frank.utils.logger import setup_logging\n",
    "from frank.constants import *\n",
    "\n",
    "## Read the config.yaml\n",
    "config = read_yaml(CONFIG_FILE_PATH)\n",
    "\n",
    "## Setup logging Configuration\n",
    "setup_logging(config)\n",
    "\n",
    "## Workflow Configuration for the main graph\n",
    "workflow_builder = WorkflowBuilder(\n",
    "    config=SimpleOakConfigGraph, \n",
    "    state_schema=SharedState,\n",
    "    checkpointer=InMemorySaver()\n",
    ")\n",
    "graph = workflow_builder.compile() # compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf21868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "user_input = \"Could you give me random movements of pickachu, and what is his evolutions?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "await print_process_astream(graph, message_input, runnable_config={\"configurable\": {\"thread_id\": \"001\"},})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "user_input = \"Please, give me random movements of that evolution?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "await print_process_astream(graph, message_input, runnable_config={\"configurable\": {\"thread_id\": \"001\"},})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1345e",
   "metadata": {},
   "source": [
    "# LangGraph + Opentelemetry tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94712ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Azure AI Inference Tracer\n",
    "import os\n",
    "from langchain_azure_ai.callbacks.tracers import AzureAIInferenceTracer\n",
    "\n",
    "## MANAGE IDENTITY INSTEAD ENV\n",
    "# from azure.ai.projects import AIProjectClient\n",
    "# from azure.identity import DefaultAzureCredential # NOTE USE ASYNC DEFAULT CREDENTIALS\n",
    "\n",
    "# project_client = AIProjectClient.from_connection_string(\n",
    "#     credential=DefaultAzureCredential(),\n",
    "#     conn_str=\"<your-project-connection-string>\",\n",
    "# )\n",
    "\n",
    "## Runnable_Config + Langchain Tracer\n",
    "langchain_tracer = AzureAIInferenceTracer(\n",
    "    connection_string=os.environ[\"AZURE_APP_INSIGHT_CONNECTION_STRING\"], # project_client.telemetry.get_connection_string()\n",
    "    enable_content_recording=True,\n",
    ")\n",
    "\n",
    "runnable_config = {\n",
    "    \"configurable\": {\"thread_id\": \"002\"}, \n",
    "    \"callbacks\": [langchain_tracer]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09288b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "user_input = \"Could you give me some random moves for Pikachu, as well as some random moves for Primeape's evolution?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "await print_process_astream(graph, message_input, runnable_config=runnable_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
