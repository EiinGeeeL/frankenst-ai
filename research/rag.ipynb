{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad32497",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01d1c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/eiingeeel/projects/frankenst-ai\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current research's directory\n",
    "research_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "# Move one directory back\n",
    "parent_dir = os.path.dirname(research_dir)\n",
    "\n",
    "# Change the current working directory to the parent directory\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "# Print the current working directory to confirm\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21a6af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.llm import LLMServices\n",
    "\n",
    "# Inicializa lo necesario (leer YAML, configurar modelos, etc.)\n",
    "LLMServices.launch()\n",
    "\n",
    "# Accede directamente al modelo configurado\n",
    "model = LLMServices.model\n",
    "embeddings = LLMServices.embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b65abc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Define Vector DB\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"pokemon_series\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./.chroma_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55473d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frank.utils.rag.unstructured import MultiVectorDocumentIndexing\n",
    "\n",
    "# Index phase\n",
    "indexing = MultiVectorDocumentIndexing(llm=model, llm_multimodal=model, vectorstore=vectorstore)\n",
    "\n",
    "indexing.load_pdf('artifacts/rag_docs/EP003 - Ash Catches a Pokémon.pdf')\n",
    "indexing.split_pdf()\n",
    "indexing.summarize_elements()\n",
    "indexing.embed_store_documents()\n",
    "retriever = indexing.get_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing.summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb63627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"In the third chapter of the Pokémon series, what is the name of the Pokémon that was flying in the daytime sky and subsequently caught?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frank.utils.rag.processing import parse_docs, show_base64_image\n",
    "\n",
    "docs_dict = parse_docs(docs)\n",
    "show_base64_image(docs_dict['images'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35524231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            context_text += text_element.text\n",
    "\n",
    "    # construct prompt with context (including images)\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question based only on the following context, which can include text, tables, and the below image.\n",
    "    Context: {context_text}\n",
    "    Question: {user_question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        for image in docs_by_type[\"images\"]:\n",
    "            prompt_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(content=prompt_content),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    \"In the third chapter of the Pokémon series, what is the name of the Pokémon that was flying in the daytime sky and subsequently caught?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46173290",
   "metadata": {},
   "source": [
    "# CUSTOM AGENTIC-ADAPTATIVE RAG GRAPH IMPLEMENTATION\n",
    "\n",
    "\n",
    "- Self-RAG: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_pinecone_movies.ipynb https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7441779b",
   "metadata": {},
   "source": [
    "¿Tiene sentido en LangGrapg el siguiente Workflow? ¿En qué casos tendría sentido esta arquitectura? ¿Qué ventajas puede tener entre hacer solo un Rag Agentic o un RAG Adaptativo?\n",
    "\n",
    "Una mezcla de RAG Agentic y Adaptativo. Es decir. Un nodo 'AgenticRAG'. Que decide contestar o ir a un ToolNode con function calling. Resulta que este ToolNode tiene dos tools que son dos retriever distintos. Uno un MultiVectorRetriever y otro un WebSearch.\n",
    "\n",
    "Después con un conditional edge GradeDocuments evaluaría el contexto recuperado para saber si es útil o no para generar la respuesta. Si no lo es. Va a un nodo rewrite question. En caso de que lo sea. Va al nodo Answer.\n",
    "\n",
    "\n",
    "El StateGraph tendría esta pinta:\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    query: str\n",
    "    retrieved_docs: list[Document] = []\n",
    "    answer: str = \"\"\n",
    "    interations: int = 0\n",
    "\n",
    "\n",
    "# LangRunnable el retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3392b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"multivector_retriever_pokemon\",\n",
    "    \"Search and return information about Pokémon series.\",\n",
    ")\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "response_model = model\n",
    "\n",
    "\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"Call the model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n",
    "    )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    ")\n",
    "\n",
    "#To structured output\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "\n",
    "\n",
    "# grader_model = model\n",
    "grader_model = model\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        grader_model\n",
    "        .with_structured_output(GradeDocuments, method=\"json_schema\").invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = 'yes' # response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\"\n",
    "    \n",
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}\n",
    "\n",
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(generate_query_or_respond)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(rewrite_question)\n",
    "workflow.add_node(generate_answer)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Who is sleeping while Caterpy and Pikachu get to know each other?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"In the charpter 3 of the pokemon series, which is the pokemon that was flying in the day sky and then capture?\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(\"Update from node\", node)\n",
    "        update[\"messages\"][-1].pretty_print()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f440f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Who is sleeping while Caterpy and Pikachu get to know each other?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "graph.invoke(message_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d76284",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"Who is sleeping while Caterpy and Pikachu get to know each other?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99105d",
   "metadata": {},
   "source": [
    "# TEST MULTIVECTOR RETRIEVER AS A TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6065cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e54df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b51029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5afa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b40d92f",
   "metadata": {},
   "source": [
    "# TEST STRUCTURED OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 1. Definir el modelo estructurado\n",
    "class Persona(BaseModel):\n",
    "    nombre: str = Field(description=\"El nombre completo de la persona\")\n",
    "    edad: int = Field(description=\"Edad de la persona\")\n",
    "    ocupacion: str = Field(description=\"Trabajo u ocupación actual\")\n",
    "\n",
    "# 2. Crear el parser\n",
    "parser = PydanticOutputParser(pydantic_object=Persona)\n",
    "\n",
    "# 3. Crear el prompt con instrucciones de formato\n",
    "system_prompt = (\n",
    "    \"Eres un extractor de datos personales.\"\n",
    "    \"siguiendo estas instrucciones:\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "# 4. Conectar con Azure OpenAI (asegúrate de configurar bien este modelo)\n",
    "llm = model\n",
    "\n",
    "# 5. Encadenar todo\n",
    "chain = prompt_template.partial(format_instructions=parser.get_format_instructions()) | llm | parser\n",
    "\n",
    "# 6. Ejecutar con mensaje de entrada\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hola, me llamo Ana Martínez, tengo 34 años y trabajo como ingeniera de software en México.\"}]\n",
    "\n",
    "\n",
    "# 7. Invocar y mostrar salida\n",
    "output = chain.invoke({\"messages\": messages})\n",
    "print(output.model_dump())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "# Definir el modelo estructurado\n",
    "class Persona(BaseModel):\n",
    "    nombre: str = Field(description=\"El nombre completo de la persona\")\n",
    "    edad: int = Field(description=\"Edad de la persona\")\n",
    "    ocupacion: str = Field(description=\"Trabajo u ocupación actual\")\n",
    "\n",
    "\n",
    "# Crear el prompt con instrucciones de formato\n",
    "system_prompt = (\n",
    "    \"Eres un extractor de datos personales.\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "# Crear el modelo con salida estructurada\n",
    "structured_llm: Runnable = model.with_structured_output(Persona, method=\"json_schema\")\n",
    "\n",
    "# Crear el pipeline completo\n",
    "chain = prompt_template | structured_llm\n",
    "\n",
    "# Ejemplo de uso\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hola, me llamo Ana Martínez, tengo 34 años y trabajo como ingeniera de software en México.\"}]\n",
    "output = chain.invoke({\"messages\": messages})\n",
    "\n",
    "print(output.model_dump())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8117dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
